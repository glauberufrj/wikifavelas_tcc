{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556ebab3",
   "metadata": {},
   "source": [
    "# Grafo Verb <-> Verb (Hiperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08b6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Célula 1: Configurações e funções carregadas.\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 1: CONFIGURAÇÃO E FUNÇÕES AUXILIARES\n",
    "# Execute esta célula uma vez para carregar as configurações e funções na memória.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import math\n",
    "import pickle # Usaremos pickle para salvar e carregar as distâncias de forma eficiente\n",
    "\n",
    "# --- 1. SEÇÃO DE CONFIGURAÇÃO CENTRALIZADA ---\n",
    "INPUT_DIR = Path('dados')\n",
    "# Ficheiro de entrada (resultado do script de coleta)\n",
    "INPUT_FILENAME = 'dados_api_20250812_125616.json' \n",
    "# Ficheiros de saída intermediários e finais\n",
    "METRICS_OUTPUT_FILENAME = 'dados_com_metricas_v2.json'\n",
    "POSITIONS_OUTPUT_FILENAME = 'dados_com_posicoes_v2.json'\n",
    "FINAL_REFINED_OUTPUT_FILENAME = 'dados_com_posicoes_v3.json'\n",
    "SHORTEST_PATHS_FILENAME = 'shortest_paths_dist.pkl' # Ficheiro para as distâncias\n",
    "\n",
    "# Parâmetros dos algoritmos\n",
    "LOUVAIN_RESOLUTION = 0.9\n",
    "RANDOM_SEED = 42\n",
    "LAYOUT_SCALE_FACTOR = 8000\n",
    "\n",
    "print(\"✅ Célula 1: Configurações e funções carregadas.\")\n",
    "\n",
    "# --- FUNÇÕES AUXILIARES ---\n",
    "\n",
    "def carregar_dados_json(filepath: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Carrega a lista de verbetes de um arquivo JSON.\"\"\"\n",
    "    print(f\"Carregando dados de '{filepath}'...\")\n",
    "    if not filepath.exists():\n",
    "        print(f\"ERRO: O arquivo de entrada não foi encontrado em '{filepath}'\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            dados = json.load(f)\n",
    "        return dados.get('verbetes_completo', [])\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ERRO: O arquivo '{filepath}' não é um JSON válido.\")\n",
    "        return []\n",
    "\n",
    "def construir_grafo(verbetes: List[Dict[str, Any]], direcionado=True) -> nx.Graph:\n",
    "    \"\"\"Constrói um grafo a partir da lista de verbetes.\"\"\"\n",
    "    print(f\"Construindo grafo {'direcionado' if direcionado else 'não-direcionado'}...\")\n",
    "    G = nx.DiGraph() if direcionado else nx.Graph()\n",
    "    titulos_ids = {v['titulo']: v['id'] for v in verbetes}\n",
    "    for verbete in verbetes:\n",
    "        G.add_node(str(verbete['id']))\n",
    "    for verbete in verbetes:\n",
    "        source_vid = str(verbete['id'])\n",
    "        for ref_titulo in verbete.get('referencias', []):\n",
    "            if ref_titulo in titulos_ids:\n",
    "                target_vid = str(titulos_ids[ref_titulo])\n",
    "                if G.has_node(source_vid) and G.has_node(target_vid):\n",
    "                    G.add_edge(source_vid, target_vid)\n",
    "    print(f\"Grafo criado com {G.number_of_nodes()} nós e {G.number_of_edges()} arestas.\")\n",
    "    return G\n",
    "\n",
    "def salvar_dados_json(data: List[Dict[str, Any]], filepath: Path):\n",
    "    \"\"\"Salva a lista de verbetes em um arquivo JSON.\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'verbetes_completo': data}, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✅ Arquivo salvo com sucesso em: '{filepath}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f572d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de 'dadosWikifavelas20250511\\dados_completos_20250812_125616.json'...\n",
      "Construindo grafo direcionado...\n",
      "Grafo criado com 3552 nós e 15301 arestas.\n",
      "\n",
      "Calculando métricas de rede complexas...\n",
      " - Detectando comunidades no componente gigante...\n",
      "Foram detectadas 19 comunidades no componente gigante.\n",
      " - Calculando Centralidades...\n",
      "Enriquecendo o dataset com as métricas calculadas...\n",
      "✅ Arquivo salvo com sucesso em: 'dadosWikifavelas20250511\\dados_com_metricas_v2.json'\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 2: CÁLCULO DE MÉTRICAS DE REDE\n",
    "# Esta célula carrega os dados brutos, calcula todas as métricas de rede\n",
    "# e salva um arquivo intermediário. Execute-a apenas quando os dados de entrada mudarem.\n",
    "\n",
    "def calcular_e_enriquecer_metricas():\n",
    "    # Executa a primeira parte do pipeline: carrega dados brutos,\n",
    "    # constrói o grafo e calcula todas as métricas de rede.\n",
    "    \n",
    "    input_path = INPUT_DIR / INPUT_FILENAME\n",
    "    metrics_output_path = INPUT_DIR / METRICS_OUTPUT_FILENAME\n",
    "\n",
    "    verbetes = carregar_dados_json(input_path)\n",
    "    if not verbetes: return\n",
    "\n",
    "    G = construir_grafo(verbetes, direcionado=True)\n",
    "    \n",
    "    print(\"\\nCalculando métricas de rede complexas...\")\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Detecção de comunidades no componente gigante\n",
    "    print(\" - Detectando comunidades no componente gigante...\")\n",
    "    connected_components = list(nx.connected_components(G_undirected))\n",
    "    partition = {}\n",
    "    if connected_components:\n",
    "        giant_component_nodes = max(connected_components, key=len)\n",
    "        G_giant = G_undirected.subgraph(giant_component_nodes)\n",
    "        partition = community_louvain.best_partition(G_giant, resolution=LOUVAIN_RESOLUTION, random_state=RANDOM_SEED)\n",
    "\n",
    "    num_communities = len(set(partition.values()))\n",
    "    print(f\"Foram detectadas {num_communities} comunidades no componente gigante.\")\n",
    "    \n",
    "    # Cores de alto contraste\n",
    "    cores_distintas = [\"#e6194B\", \"#3cb44b\", \"#ffe119\", \"#4363d8\", \"#f58231\", \"#911eb4\", \"#46f0f0\", \"#f032e6\", \"#bcf60c\", \"#fabebe\", \"#008080\", \"#e6beff\", \"#9A6324\", \"#fffac8\", \"#800000\", \"#aaffc3\", \"#808000\", \"#ffd8b1\", \"#000075\", \"#a9a9a9\"]\n",
    "    random.seed(RANDOM_SEED)\n",
    "    community_colors = {i: cores_distintas[i % len(cores_distintas)] for i in range(num_communities)}\n",
    "\n",
    "    # Cálculo de centralidades\n",
    "    print(\" - Calculando Centralidades...\")\n",
    "    metrics = {\n",
    "        'partition': partition,\n",
    "        'in_degree': dict(G.in_degree()),\n",
    "        'out_degree': dict(G.out_degree()),\n",
    "        'total_degree': dict(G.degree()),\n",
    "        'betweenness': nx.betweenness_centrality(G),\n",
    "        'pagerank': nx.pagerank(G),\n",
    "        'closeness': nx.closeness_centrality(G),\n",
    "        'clustering': nx.clustering(G_undirected)\n",
    "    }\n",
    "\n",
    "    # Enriquecimento dos dados\n",
    "    print(\"Enriquecendo o dataset com as métricas calculadas...\")\n",
    "    for verbete in verbetes:\n",
    "        vid = str(verbete['id'])\n",
    "        community_id = partition.get(vid, -1)\n",
    "        verbete.update({\n",
    "            'community_id': community_id,\n",
    "            'community_color': community_colors.get(community_id, '#6A737D'),\n",
    "            'in_degree': metrics['in_degree'].get(vid, 0),\n",
    "            'out_degree': metrics['out_degree'].get(vid, 0),\n",
    "            'total_degree': metrics['total_degree'].get(vid, 0),\n",
    "            'betweenness_centrality': metrics['betweenness'].get(vid, 0.0),\n",
    "            'pagerank': metrics['pagerank'].get(vid, 0.0),\n",
    "            'clustering_coefficient': metrics['clustering'].get(vid, 0.0),\n",
    "            'closeness_centrality': metrics['closeness'].get(vid, 0.0)\n",
    "        })\n",
    "        \n",
    "    salvar_dados_json(verbetes, metrics_output_path)\n",
    "\n",
    "# Executa o cálculo de métricas\n",
    "calcular_e_enriquecer_metricas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a8fd82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de 'dadosWikifavelas20250511\\dados_com_metricas_v2.json'...\n",
      "Construindo grafo não-direcionado...\n",
      "Grafo criado com 3552 nós e 14128 arestas.\n",
      "Carregando distâncias pré-calculadas de 'dadosWikifavelas20250511\\shortest_paths_dist.pkl'...\n",
      " - Calculando Kamada-Kawai para 3337 nós...\n",
      " - Posicionando 215 nós isolados em grelha...\n",
      "✅ Arquivo salvo com sucesso em: 'dadosWikifavelas20250511\\dados_com_posicoes_v2.json'\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 3: CÁLCULO DE LAYOUT INICIAL (KAMADA-KAWAI + GRELHA)\n",
    "# Esta célula carrega os dados com métricas e calcula as posições iniciais dos nós.\n",
    "# A otimização do 'dist' fará com que ela seja muito mais rápida após a primeira execução.\n",
    "\n",
    "def calcular_e_salvar_layout_inicial():\n",
    "    # Carrega os dados com métricas e calcula as posições iniciais dos nós,\n",
    "    # otimizando o cálculo de distâncias.\n",
    "    \n",
    "    metrics_path = INPUT_DIR / METRICS_OUTPUT_FILENAME\n",
    "    positions_output_path = INPUT_DIR / POSITIONS_OUTPUT_FILENAME\n",
    "    shortest_paths_path = INPUT_DIR / SHORTEST_PATHS_FILENAME\n",
    "    \n",
    "    verbetes = carregar_dados_json(metrics_path)\n",
    "    if not verbetes: return\n",
    "\n",
    "    # Reconstrói o grafo não-direcionado para componentes e distâncias\n",
    "    G_undirected = construir_grafo(verbetes, direcionado=False)\n",
    "\n",
    "    # Separa componente gigante dos isolados\n",
    "    connected_components = list(nx.connected_components(G_undirected))\n",
    "    giant_component_nodes = max(connected_components, key=len)\n",
    "    G_giant = G_undirected.subgraph(giant_component_nodes)\n",
    "    isolated_nodes = [node for node in G_undirected.nodes() if node not in giant_component_nodes]\n",
    "\n",
    "    # Otimização: Calcula ou carrega as distâncias do caminho mais curto\n",
    "    if shortest_paths_path.exists():\n",
    "        print(f\"Carregando distâncias pré-calculadas de '{shortest_paths_path}'...\")\n",
    "        with open(shortest_paths_path, 'rb') as f:\n",
    "            shortest_path_dist = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Calculando distâncias de caminho mais curto (pode demorar)...\")\n",
    "        path_length = nx.all_pairs_shortest_path_length(G_giant)\n",
    "        shortest_path_dist = {source: targets for source, targets in path_length}\n",
    "        with open(shortest_paths_path, 'wb') as f:\n",
    "            pickle.dump(shortest_path_dist, f)\n",
    "        print(f\"Distâncias salvas em '{shortest_paths_path}' para uso futuro.\")\n",
    "\n",
    "    # Calcula layout do componente gigante usando as distâncias pré-calculadas\n",
    "    print(f\" - Calculando Kamada-Kawai para {G_giant.number_of_nodes()} nós...\")\n",
    "    posicoes_relativas = nx.kamada_kawai_layout(G_giant, dist=shortest_path_dist, scale=LAYOUT_SCALE_FACTOR)\n",
    "    \n",
    "    posicoes_finais = {node_id: {'x': pos[0], 'y': pos[1]} for node_id, pos in posicoes_relativas.items()}\n",
    "    \n",
    "    # Posiciona os nós isolados em grelha\n",
    "    print(f\" - Posicionando {len(isolated_nodes)} nós isolados em grelha...\")\n",
    "    if isolated_nodes:\n",
    "        max_x_main_graph = max(pos['x'] for pos in posicoes_finais.values()) if posicoes_finais else 0\n",
    "        grid_start_x = max_x_main_graph + (LAYOUT_SCALE_FACTOR * 0.3)\n",
    "        spacing = 300\n",
    "        cols = int(math.sqrt(len(isolated_nodes))) + 1\n",
    "        for i, node_id in enumerate(isolated_nodes):\n",
    "            row = i // cols\n",
    "            col = i % cols\n",
    "            posicoes_finais[node_id] = {'x': grid_start_x + (col * spacing), 'y': row * spacing}\n",
    "\n",
    "    # Adiciona posições ao dataset\n",
    "    for verbete in verbetes:\n",
    "        verbete['position'] = posicoes_finais.get(str(verbete['id']), {'x': 0, 'y': 0})\n",
    "        \n",
    "    salvar_dados_json(verbetes, positions_output_path)\n",
    "\n",
    "# Executa o cálculo de layout inicial\n",
    "calcular_e_salvar_layout_inicial()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90fe94a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de 'dadosWikifavelas20250511\\dados_com_posicoes_v2.json'...\n",
      "Construindo grafo não-direcionado...\n",
      "Grafo criado com 3552 nós e 14128 arestas.\n",
      "\n",
      "Iniciando o refinamento do layout para evitar sobreposição...\n",
      " - Refinando 3337 nós do componente gigante...\n",
      "✅ Arquivo salvo com sucesso em: 'dadosWikifavelas20250511\\dados_com_posicoes_v3.json'\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 4: REFINAMENTO DO LAYOUT PARA EVITAR SOBREPOSIÇÃO\n",
    "# Esta célula é ideal para ser executada várias vezes, ajustando os parâmetros\n",
    "# do spring_layout para encontrar a melhor visualização.\n",
    "\n",
    "def refinar_e_salvar_layout_final():\n",
    "    #Carrega os dados com posições iniciais e aplica o spring_layout\n",
    "    # para refinar o posicionamento e evitar sobreposição.\n",
    "    \n",
    "    positions_path = INPUT_DIR / POSITIONS_OUTPUT_FILENAME\n",
    "    final_output_path = INPUT_DIR / FINAL_REFINED_OUTPUT_FILENAME\n",
    "    \n",
    "    verbetes = carregar_dados_json(positions_path)\n",
    "    if not verbetes: return\n",
    "\n",
    "    G_undirected = construir_grafo(verbetes, direcionado=False)\n",
    "\n",
    "    print(\"\\nIniciando o refinamento do layout para evitar sobreposição...\")\n",
    "    posicoes_iniciais_scaled = {str(v['id']): (v['position']['x'], v['position']['y']) for v in verbetes if 'position' in v}\n",
    "\n",
    "    # Normaliza as posições para a escala ideal do spring_layout [-1, 1]\n",
    "    max_abs_coord = max(abs(coord) for pos in posicoes_iniciais_scaled.values() for coord in pos)\n",
    "    if max_abs_coord == 0: max_abs_coord = 1.0\n",
    "    posicoes_iniciais_unscaled = {node: (x / max_abs_coord, y / max_abs_coord) for node, (x, y) in posicoes_iniciais_scaled.items()}\n",
    "\n",
    "    # Identifica o componente gigante novamente\n",
    "    connected_components = list(nx.connected_components(G_undirected))\n",
    "    giant_component_nodes = max(connected_components, key=len)\n",
    "    G_giant = G_undirected.subgraph(giant_component_nodes)\n",
    "    \n",
    "    pos_giant_iniciais_unscaled = {node: pos for node, pos in posicoes_iniciais_unscaled.items() if node in giant_component_nodes}\n",
    "\n",
    "    print(f\" - Refinando {len(pos_giant_iniciais_unscaled)} nós do componente gigante...\")\n",
    "    pos_ajustadas_giant_unscaled = nx.spring_layout(\n",
    "        G_giant, \n",
    "        pos=pos_giant_iniciais_unscaled, \n",
    "        iterations=80,\n",
    "        seed=RANDOM_SEED,\n",
    "        k=0.4 / math.sqrt(G_giant.number_of_nodes()) # Parâmetro 'k' para ajustar o espaçamento\n",
    "    )\n",
    "\n",
    "    # Combina e re-escala as posições\n",
    "    posicoes_finais_refinadas = {}\n",
    "    for node_id, pos_tuple_scaled in posicoes_iniciais_scaled.items():\n",
    "        if node_id in pos_ajustadas_giant_unscaled:\n",
    "            final_pos_unscaled = pos_ajustadas_giant_unscaled[node_id]\n",
    "            posicoes_finais_refinadas[node_id] = {'x': float(final_pos_unscaled[0] * max_abs_coord), 'y': float(final_pos_unscaled[1] * max_abs_coord)}\n",
    "        else: # Mantém a posição original para os nós isolados\n",
    "            posicoes_finais_refinadas[node_id] = {'x': float(pos_tuple_scaled[0]), 'y': float(pos_tuple_scaled[1])}\n",
    "            \n",
    "    # Atualiza o dataset final\n",
    "    for verbete in verbetes:\n",
    "        verbete_id = str(verbete['id'])\n",
    "        if verbete_id in posicoes_finais_refinadas:\n",
    "            verbete['position'] = posicoes_finais_refinadas[verbete_id]\n",
    "    \n",
    "    salvar_dados_json(verbetes, final_output_path)\n",
    "\n",
    "# Executa o refinamento final\n",
    "refinar_e_salvar_layout_final()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4680ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular_metrica_composta.py\n",
    "#\n",
    "# Este script lê os dados finais da rede e calcula uma métrica composta\n",
    "# para identificar os \"super-nós\" — verbetes que são importantes em múltiplas dimensões.\n",
    "#\n",
    "# Metodologia Estatística:\n",
    "# 1. Normalização Min-Max: Cada uma das cinco métricas selecionadas é normalizada\n",
    "#    para uma escala de 0 a 1. Isso garante que cada métrica contribua de forma\n",
    "#    equitativa para o resultado final, independentemente da sua escala original.\n",
    "#    Fórmula: valor_normalizado = (valor - min) / (max - min)\n",
    "# 2. Soma Ponderada (com pesos iguais): A métrica composta é a soma das\n",
    "#    métricas normalizadas. Esta abordagem é mais robusta que a multiplicação,\n",
    "#    pois não anula a pontuação de um nó que seja fraco em uma única dimensão.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_metrica_composta():\n",
    "    \"\"\"\n",
    "    Calcula uma métrica de importância composta normalizando e somando\n",
    "    diferentes indicadores de centralidade e atividade.\n",
    "    \"\"\"\n",
    "    # --- 1. CONFIGURAÇÃO DE ARQUIVOS ---\n",
    "    INPUT_DIR = Path('dadosWikifavelas20250511')\n",
    "    INPUT_FILENAME = 'dados_com_posicoes_v3.json'\n",
    "    OUTPUT_FILENAME = 'dados_com_metrica_composta.json'\n",
    "    \n",
    "    input_path = INPUT_DIR / INPUT_FILENAME\n",
    "    output_path = INPUT_DIR / OUTPUT_FILENAME\n",
    "\n",
    "    if not input_path.exists():\n",
    "        print(f\"ERRO: O arquivo de entrada não foi encontrado em '{input_path}'\")\n",
    "        return\n",
    "\n",
    "    # --- 2. CARREGAMENTO E PREPARAÇÃO DOS DADOS ---\n",
    "    print(f\"Carregando dados de '{input_path}'...\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        dados = json.load(f)\n",
    "    \n",
    "    verbetes = dados.get('verbetes_completo', [])\n",
    "    if not verbetes:\n",
    "        print(\"Nenhum verbete encontrado no arquivo.\")\n",
    "        return\n",
    "        \n",
    "    # Usar o Pandas facilita muito a normalização de colunas\n",
    "    df = pd.DataFrame(verbetes)\n",
    "    \n",
    "    # Lista das métricas que farão parte do índice composto\n",
    "    metricas_para_normalizar = [\n",
    "        'betweenness_centrality',\n",
    "        'closeness_centrality',\n",
    "        'total_degree',\n",
    "        'quantidade_edicoes',\n",
    "        'pagerank'\n",
    "    ]\n",
    "\n",
    "    print(\"\\nNormalizando as métricas para uma escala de 0 a 1...\")\n",
    "    for metrica in metricas_para_normalizar:\n",
    "        if metrica in df.columns:\n",
    "            min_val = df[metrica].min()\n",
    "            max_val = df[metrica].max()\n",
    "            \n",
    "            # Evita divisão por zero se todos os valores forem iguais\n",
    "            if (max_val - min_val) > 0:\n",
    "                df[f'{metrica}_norm'] = (df[metrica] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                df[f'{metrica}_norm'] = 0 # Se não há variação, a contribuição é nula\n",
    "            \n",
    "            print(f\" - Métrica '{metrica}' normalizada.\")\n",
    "        else:\n",
    "            print(f\" - AVISO: Métrica '{metrica}' não encontrada no dataset.\")\n",
    "            df[f'{metrica}_norm'] = 0 # Adiciona uma coluna de zeros se a métrica não existir\n",
    "\n",
    "    # --- 3. CÁLCULO DA MÉTRICA COMPOSTA ---\n",
    "    print(\"\\nCalculando a métrica composta pela soma dos valores normalizados...\")\n",
    "    \n",
    "    # A métrica composta é a soma das versões normalizadas de cada indicador\n",
    "    df['metrica_composta'] = (\n",
    "        df['betweenness_centrality_norm'] +\n",
    "        df['closeness_centrality_norm'] +\n",
    "        df['total_degree_norm'] +\n",
    "        df['quantidade_edicoes_norm'] +\n",
    "        df['pagerank_norm']\n",
    "    )\n",
    "    \n",
    "    print(\"Métrica composta calculada com sucesso.\")\n",
    "\n",
    "    # --- 4. ANÁLISE DOS RESULTADOS ---\n",
    "    print(\"\\n--- TOP 20 VERBETES POR MÉTRICA COMPOSTA ---\")\n",
    "    \n",
    "    # Ordena o DataFrame pela nova métrica para encontrar os nós mais importantes\n",
    "    df_sorted = df.sort_values(by='metrica_composta', ascending=False)\n",
    "    \n",
    "    for index, row in df_sorted.head(20).iterrows():\n",
    "        print(f\"  - Título: {row['titulo']:<70} | Pontuação Composta: {row['metrica_composta']:.4f}\")\n",
    "\n",
    "    # --- 5. SALVANDO O ARQUIVO FINAL ---\n",
    "    # Converte o DataFrame de volta para o formato de dicionário original\n",
    "    verbetes_finais = df_sorted.to_dict(orient='records')\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'verbetes_completo': verbetes_finais}, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    print(f\"\\n✅ Arquivo com a métrica composta salvo em '{output_path}'!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    calcular_metrica_composta()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8d387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de 'dadosWikifavelas20250511\\dados_com_metrica_composta.json'...\n",
      "Reconstruindo o grafo a partir dos dados...\n",
      "Grafo reconstruído com 3552 nós e 15301 arestas.\n",
      "\n",
      "Calculando a métrica de Restrição (Constraint) para cada nó...\n",
      "Cálculo da métrica de Constraint concluído.\n",
      "Adicionando a nova métrica ao conjunto de dados...\n",
      "\n",
      "✅ Arquivo com a métrica de Constraint salvo em 'dadosWikifavelas20250511\\dados_com_constraint.json'!\n",
      "Lembre-se: nós com BAIXO valor de 'constraint' são os mais interessantes para análise de 'buracos estruturais'.\n"
     ]
    }
   ],
   "source": [
    "# calcular_constraint.py\n",
    "#\n",
    "# Este script lê os dados enriquecidos da rede, reconstrói o grafo\n",
    "# e calcula a métrica de Restrição (Constraint) de Burt para cada nó.\n",
    "# A métrica é então adicionada ao dataset, que é salvo em um novo arquivo.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def calcular_constraint_para_nos():\n",
    "    \"\"\"\n",
    "    Calcula a métrica de Restrição (Constraint) e a adiciona ao\n",
    "    conjunto de dados de verbetes.\n",
    "    \"\"\"\n",
    "    # --- 1. CONFIGURAÇÃO DE ARQUIVOS ---\n",
    "    INPUT_DIR = Path('dadosWikifavelas20250511')\n",
    "    \n",
    "    # Ficheiro de entrada que já contém todas as outras métricas\n",
    "    INPUT_FILENAME = 'dados_com_metrica_composta.json'\n",
    "    \n",
    "    # Novo ficheiro de saída que incluirá a métrica de constraint\n",
    "    OUTPUT_FILENAME = 'dados_com_constraint.json'\n",
    "    \n",
    "    input_path = INPUT_DIR / INPUT_FILENAME\n",
    "    output_path = INPUT_DIR / OUTPUT_FILENAME\n",
    "\n",
    "    if not input_path.exists():\n",
    "        print(f\"ERRO: O arquivo de entrada não foi encontrado em '{input_path}'\")\n",
    "        return\n",
    "\n",
    "    # --- 2. CARREGAMENTO DOS DADOS ---\n",
    "    print(f\"Carregando dados de '{input_path}'...\")\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            dados = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ERRO: O arquivo '{input_path}' não é um JSON válido.\")\n",
    "        return\n",
    "    \n",
    "    verbetes = dados.get('verbetes_completo', [])\n",
    "    if not verbetes:\n",
    "        print(\"Nenhum verbete encontrado no arquivo.\")\n",
    "        return\n",
    "        \n",
    "    # --- 3. RECONSTRUÇÃO DO GRAFO ---\n",
    "    print(\"Reconstruindo o grafo a partir dos dados...\")\n",
    "    # Usamos um DiGraph (direcionado) pois a constraint em NetworkX considera\n",
    "    # as ligações de saída.\n",
    "    G = nx.DiGraph()\n",
    "    titulos_ids = {v['titulo']: v['id'] for v in verbetes}\n",
    "    \n",
    "    # Adiciona todos os nós\n",
    "    for verbete in verbetes:\n",
    "        G.add_node(str(verbete['id']))\n",
    "        \n",
    "    # Adiciona todas as arestas\n",
    "    for verbete in verbetes:\n",
    "        source_vid = str(verbete['id'])\n",
    "        for ref_titulo in verbete.get('referencias', []):\n",
    "            if ref_titulo in titulos_ids:\n",
    "                target_vid = str(titulos_ids[ref_titulo])\n",
    "                if G.has_node(source_vid) and G.has_node(target_vid):\n",
    "                    G.add_edge(source_vid, target_vid)\n",
    "    \n",
    "    print(f\"Grafo reconstruído com {G.number_of_nodes()} nós e {G.number_of_edges()} arestas.\")\n",
    "\n",
    "    # --- 4. CÁLCULO DA MÉTRICA DE CONSTRAINT ---\n",
    "    print(\"\\nCalculando a métrica de Restrição (Constraint) para cada nó...\")\n",
    "    # O cálculo pode levar alguns momentos dependendo do tamanho do grafo\n",
    "    constraint_scores = nx.constraint(G)\n",
    "    print(\"Cálculo da métrica de Constraint concluído.\")\n",
    "\n",
    "    # --- 5. ENRIQUECIMENTO DOS DADOS ---\n",
    "    print(\"Adicionando a nova métrica ao conjunto de dados...\")\n",
    "    for verbete in verbetes:\n",
    "        verbete_id = str(verbete['id'])\n",
    "        # Adiciona o valor da constraint ao dicionário de cada verbete\n",
    "        # Um valor BAIXO de constraint é o que indica um \"nó-ponte\"\n",
    "        verbete['constraint'] = constraint_scores.get(verbete_id, None)\n",
    "\n",
    "    # --- 6. SALVANDO O ARQUIVO FINAL ---\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'verbetes_completo': verbetes}, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    print(f\"\\n✅ Arquivo com a métrica de Constraint salvo em '{output_path}'!\")\n",
    "    print(\"Lembre-se: nós com BAIXO valor de 'constraint' são os mais interessantes para análise de 'buracos estruturais'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    calcular_constraint_para_nos()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
